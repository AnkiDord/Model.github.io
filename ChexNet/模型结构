# 损失函数：交叉熵损失函数(CrossEntropy Loss) 

# https://zhuanlan.zhihu.com/p/98785902 知乎上写的简单易懂

信息量：它是用来衡量一个事件的不确定性的；一个事件发生的概率越大，不确定性越小，则它所携带的信息量就越小。

熵：它是用来衡量一个系统的混乱程度的，代表一个系统中信息量的总和；信息量总和越大，表明这个系统不确定性就越大。

交叉熵：交叉熵：它主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出， 
       H(p,q)为交叉熵：
                            H(p,q)=-∑(p(x)logq(x)+(1-p(x))log(1-q(x)))
       其中x从0到N-1(N即是期望结果长度)
       
在pythorch中CrossEntropyLoss()为H(p,q)=-∑p(x)logq(x), Pytorch中CrossEntropyLoss()函数的主要是将softmax-log-NLLLoss合并到一块得到的结果。

    1、Softmax后的数值都在0~1之间，所以ln之后值域是负无穷到0。

    2、然后将Softmax之后的结果取log，将乘法改成加法减少计算量，同时保障函数的单调性 。

    3、NLLLoss的结果就是把上面的输出与Label对应的那个值拿出来，去掉负号，再求均值。
    
NLLoss：就是把经过log_softmax函数的值与标签（Label）对应的那个值拿出来相加求和，再求均值，最后在求相反数，也就是添加负号。

# 分类用的sigmoid函数：1/(1+(e-x)), 其中(e-x)次幂。

# 优化函数Adam(下周会学习优化函数)

# CheXNet(DenseNet121)
CheXNet由PreResInitBlock(卷积块第一次下采样), pooling层(第二次下采样), 3个TransitionBlock(3次下采样)，和58个DenseBlock，以及PreResActivation(BN，ReLU), AvgPool, Linear组成

# CheXNet各卷积层输出层数
[[96, 128, 160, 192, 224, 256],
 [160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 480, 512],
 [288, 320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800, 832, 864, 896, 928, 960, 992, 1024],
 [544, 576, 608, 640, 672, 704, 736, 768, 800, 832, 864, 896, 928, 960, 992, 1024]]
 
 DenseNet其它系列只是网络深度不同

        
